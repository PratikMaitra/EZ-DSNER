{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EZ-DSNER — Unified DS-NER Experiment Notebook\n",
    "\n",
    "**Pipeline:**  \n",
    "1. Setup & dependency check  \n",
    "2. Data preparation (format conversion)  \n",
    "3. Training & inference (CuPuL + others)  \n",
    "4. Post-processing  \n",
    "5. Evaluation & comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prati/anaconda3/envs/dsner/bin/python\n",
      "3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n",
      "Requirement already satisfied: seqeval==1.2.2 in /home/prati/anaconda3/envs/dsner/lib/python3.7/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/prati/anaconda3/envs/dsner/lib/python3.7/site-packages (from seqeval==1.2.2) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/prati/anaconda3/envs/dsner/lib/python3.7/site-packages (from seqeval==1.2.2) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/prati/anaconda3/envs/dsner/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/prati/anaconda3/envs/dsner/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/prati/anaconda3/envs/dsner/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (3.1.0)\n",
      "seqeval: ok\n",
      "AdamW: ok\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "!pip install seqeval==1.2.2\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from transformers import AdamW\n",
    "print(\"seqeval: ok\")\n",
    "print(\"AdamW: ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT : /home/prati/A2_Dist_NER_Agro/original_dsner\n",
      "DATA_DIR     : /home/prati/A2_Dist_NER_Agro/original_dsner/data/QTL\n",
      "WRAPPER_DIR  : /home/prati/A2_Dist_NER_Agro/original_dsner\n",
      "PRED_DIR     : /home/prati/A2_Dist_NER_Agro/original_dsner/predictions\n",
      "GPU          : 0\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, glob\n",
    "\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "# AUTO-DETECT PATHS\n",
    "# The notebook assumes it lives inside the project root alongside\n",
    "# dsner_wrapper.py, dsner_data.py, dsner_postprocess.py, and the\n",
    "# method directories (ATSEN/, BOND/, CuPuL/, etc.).\n",
    "#\n",
    "# Expected layout:\n",
    "#   <project_root>/\n",
    "#   ├── dsner_wrapper.py\n",
    "#   ├── dsner_data.py\n",
    "#   ├── dsner_postprocess.py\n",
    "#   ├── dsner_jupyter.ipynb        ← this notebook\n",
    "#   ├── data/\n",
    "#   │   └── QTL/                   ← (auto-detected)\n",
    "#   │       ├── train.txt / train_ALL.txt\n",
    "#   │       ├── test.txt\n",
    "#   │       ├── types.txt\n",
    "#   │       └── valid.txt\n",
    "#   ├── ATSEN/\n",
    "#   ├── BOND/\n",
    "#   ├── CuPuL/\n",
    "#   └── ...\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "# Detect project root: walk up from notebook location until we find dsner_wrapper.py\n",
    "def find_project_root():\n",
    "    \"\"\"Find the project root by looking for dsner_wrapper.py.\"\"\"\n",
    "    # Try current working directory first\n",
    "    candidates = [os.getcwd()]\n",
    "    # Also try the notebook's own directory if running in Jupyter\n",
    "    try:\n",
    "        nb_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "        candidates.append(nb_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Walk up from each candidate\n",
    "    for start in candidates:\n",
    "        d = start\n",
    "        for _ in range(5):  # max 5 levels up\n",
    "            if os.path.exists(os.path.join(d, 'dsner_wrapper.py')):\n",
    "                return d\n",
    "            d = os.path.dirname(d)\n",
    "    return os.getcwd()  # fallback\n",
    "\n",
    "def find_data_dir(project_root):\n",
    "    \"\"\"Find the QTL data directory containing train/test/types files.\"\"\"\n",
    "    data_base = os.path.join(project_root, 'data')\n",
    "    if not os.path.isdir(data_base):\n",
    "        return None\n",
    "    # Search for a directory containing types.txt + test.txt\n",
    "    for dirpath, dirnames, filenames in os.walk(data_base):\n",
    "        if 'types.txt' in filenames and 'test.txt' in filenames:\n",
    "            # Check it has some train file too\n",
    "            if any(f in filenames for f in ['train.txt', 'train_ALL.txt']):\n",
    "                return dirpath\n",
    "    return None\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR     = find_data_dir(PROJECT_ROOT)\n",
    "WRAPPER_DIR  = PROJECT_ROOT  # wrapper files live in project root\n",
    "DATASET      = \"qtl\"\n",
    "GPU          = \"0\"\n",
    "\n",
    "# Add wrapper to Python path\n",
    "if WRAPPER_DIR not in sys.path:\n",
    "    sys.path.insert(0, WRAPPER_DIR)\n",
    "\n",
    "# Derived paths\n",
    "GOLD_TEST  = os.path.join(DATA_DIR, \"test.txt\") if DATA_DIR else None\n",
    "TYPES_FILE = os.path.join(DATA_DIR, \"types.txt\") if DATA_DIR else None\n",
    "PRED_DIR   = os.path.join(PROJECT_ROOT, \"predictions\")\n",
    "os.makedirs(PRED_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PROJECT_ROOT : {PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR     : {DATA_DIR}\")\n",
    "print(f\"WRAPPER_DIR  : {WRAPPER_DIR}\")\n",
    "print(f\"PRED_DIR     : {PRED_DIR}\")\n",
    "print(f\"GPU          : {GPU}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wrapper files ===\n",
      "  ✓ dsner_wrapper.py\n",
      "  ✓ dsner_data.py\n",
      "  ✓ dsner_postprocess.py\n",
      "\n",
      "=== Data files ===\n",
      "  ✓ README.md                                 18 lines\n",
      "  ✓ data_processor.py                        185 lines\n",
      "  ✓ dict_match.py                            247 lines\n",
      "  ✓ pred_test_balance.txt                 33,267 lines\n",
      "  ✓ pred_test_high_recall.txt             33,268 lines\n",
      "  ✓ test.txt                              33,295 lines\n",
      "  ✓ train.txt                            532,882 lines\n",
      "  ✓ types.txt                                  2 lines\n",
      "  ✓ valid.txt                              1,096 lines\n",
      "\n",
      "=== Method directories ===\n",
      "  ✓ ATSEN\n",
      "  ✓ AutoNER\n",
      "  ✓ BOND\n",
      "  ✓ CuPuL\n",
      "  ✓ DeSERT\n",
      "  ✓ mproto\n",
      "  ✓ RoSTER\n",
      "  ✓ SCDL\n"
     ]
    }
   ],
   "source": [
    "# Verify everything exists\n",
    "print(\"=== Wrapper files ===\")\n",
    "for f in [\"dsner_wrapper.py\", \"dsner_data.py\", \"dsner_postprocess.py\"]:\n",
    "    path = os.path.join(WRAPPER_DIR, f)\n",
    "    ok = \"✓\" if os.path.exists(path) else \"✗ MISSING\"\n",
    "    print(f\"  {ok} {f}\")\n",
    "\n",
    "print(\"\\n=== Data files ===\")\n",
    "if DATA_DIR:\n",
    "    for f in sorted(os.listdir(DATA_DIR)):\n",
    "        fpath = os.path.join(DATA_DIR, f)\n",
    "        if os.path.isfile(fpath):\n",
    "            n = sum(1 for _ in open(fpath, errors='replace'))\n",
    "            print(f\"  ✓ {f:35s} {n:>8,} lines\")\n",
    "else:\n",
    "    print(\"  ✗ DATA_DIR not found! Check data/ directory.\")\n",
    "\n",
    "print(\"\\n=== Method directories ===\")\n",
    "for m in [\"ATSEN\", \"AutoNER\", \"BOND\", \"CuPuL\", \"DeSERT\", \"mproto\", \"RoSTER\", \"SCDL\"]:\n",
    "    path = os.path.join(PROJECT_ROOT, m)\n",
    "    ok = \"✓\" if os.path.isdir(path) else \"✗\"\n",
    "    print(f\"  {ok} {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity types: ['Trait', 'Gene']\n"
     ]
    }
   ],
   "source": [
    "# Read entity types\n",
    "with open(TYPES_FILE) as f:\n",
    "    TYPES = [line.strip() for line in f if line.strip()]\n",
    "print(f\"Entity types: {TYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preparation\n",
    "\n",
    "Convert QTL data into the format each method expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format                 Description\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  conll_bio            TOKEN BIO_TAG                  (2-col, standard CoNLL)\n",
      "  conll_dist           TOKEN O DIST_LABEL             (3-col, distant-labeled train)\n",
      "  conll_bio_dist       TOKEN BIO_TAG DIST_LABEL       (3-col, bio + distant)\n",
      "  bond_json            [{\"str_words\":[], \"tags\":[]}]  (BOND/ATSEN/SCDL/DeSERT)\n",
      "  roster               train_text.txt + label file     (RoSTER directory)\n",
      "  autoner_raw          one token per line, no labels   (AutoNER raw_text.txt)\n",
      "  autoner_ck           TOKEN TIE_BREAK TYPE            (AutoNER .ck truth file)\n",
      "  mproto_jsonl         {\"tokens\":[], \"entities\":[]}    (MProto, one JSON/line)\n",
      "\n",
      "Method       Train format         Test format\n",
      "───────────────────────────────────────────────────────\n",
      "  BOND         bond_json            bond_json\n",
      "  ATSEN        bond_json            bond_json\n",
      "  SCDL         bond_json            bond_json\n",
      "  DeSERT       bond_json            bond_json\n",
      "  RoSTER       roster               roster\n",
      "  CuPuL        conll_dist           conll_bio\n",
      "  AutoNER      autoner_raw          autoner_ck\n",
      "  MProto       mproto_jsonl         mproto_jsonl\n"
     ]
    }
   ],
   "source": [
    "from dsner_data import DataPreparer, FormatConverter, SUPPORTED_FORMATS, METHOD_FORMATS\n",
    "\n",
    "print(\"Format                 Description\")\n",
    "print(\"─\" * 70)\n",
    "for name, desc in SUPPORTED_FORMATS.items():\n",
    "    print(f\"  {name:20s} {desc}\")\n",
    "\n",
    "print(\"\\nMethod       Train format         Test format\")\n",
    "print(\"─\" * 55)\n",
    "for method, fmts in METHOD_FORMATS.items():\n",
    "    print(f\"  {method:10s}   {fmts['train']:18s}   {fmts['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 13:12:22] INFO dsner_data: Auto-detected format: conll_dist\n",
      "[2026-02-27 13:12:22] INFO dsner_data: read_conll_dist: 18706 sentences from /home/prati/A2_Dist_NER_Agro/original_dsner/data/QTL/train.txt\n",
      "[2026-02-27 13:12:22] INFO dsner_data: Auto-detected format: conll_bio\n",
      "[2026-02-27 13:12:22] INFO dsner_data: read_conll_bio: 1045 sentences from /home/prati/A2_Dist_NER_Agro/original_dsner/data/QTL/test.txt\n",
      "[2026-02-27 13:12:22] INFO dsner_data: Auto-detected format: conll_bio\n",
      "[2026-02-27 13:12:22] INFO dsner_data: read_conll_bio: 25 sentences from /home/prati/A2_Dist_NER_Agro/original_dsner/data/QTL/valid.txt\n",
      "[2026-02-27 13:12:22] INFO dsner_data: DataPreparer: qtl  types=['Trait', 'Gene']  train=18706  test=1045  dev=25\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_bond_json: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/ATSEN/dataset/qtl/qtl_train.json\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_bond_json: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/ATSEN/dataset/qtl/qtl_test.json\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_bond_json: 25 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/ATSEN/dataset/qtl/qtl_dev.json\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_autoner_raw: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/AutoNER/data/qtl/raw_text.txt\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_autoner_ck: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/AutoNER/data/qtl/truth_test.ck\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_autoner_ck: 25 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/AutoNER/data/qtl/truth_dev.ck\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_bond_json: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/BOND/dataset/qtl/train.json\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_bond_json: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/BOND/dataset/qtl/test.json\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_bond_json: 25 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/BOND/dataset/qtl/dev.json\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_conll_dist: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl/train.ALL.txt\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_conll_bio: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl/train.txt\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_conll_bio: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl/test.txt\n",
      "[2026-02-27 13:12:23] INFO dsner_data: write_conll_bio: 25 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl/dev.txt\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_bond_json: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/DeSERT/dataset/qtl/qtl_train.json\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_bond_json: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/DeSERT/dataset/qtl/qtl_test.json\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_bond_json: 25 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/DeSERT/dataset/qtl/qtl_dev.json\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_mproto_jsonl: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/mproto/data/qtl/train.jsonl\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_mproto_jsonl: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/mproto/data/qtl/test.jsonl\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_mproto_jsonl: 25 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/mproto/data/qtl/dev.jsonl\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_roster: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/RoSTER/data/qtl (train)\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_roster: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/RoSTER/data/qtl (test)\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_bond_json: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/SCDL/dataset/qtl_train.json\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_bond_json: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/SCDL/dataset/qtl_test.json\n",
      "[2026-02-27 13:12:24] INFO dsner_data: write_bond_json: 25 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/SCDL/dataset/qtl_dev.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data preparation results:\n",
      "════════════════════════════════════════════════════════════\n",
      "  ✓ ATSEN      → /home/prati/A2_Dist_NER_Agro/original_dsner/ATSEN/dataset/qtl\n",
      "  ✓ AutoNER    → /home/prati/A2_Dist_NER_Agro/original_dsner/AutoNER/data/qtl\n",
      "  ✓ BOND       → /home/prati/A2_Dist_NER_Agro/original_dsner/BOND/dataset/qtl\n",
      "  ✓ CuPuL      → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl\n",
      "  ✓ DeSERT     → /home/prati/A2_Dist_NER_Agro/original_dsner/DeSERT/dataset/qtl\n",
      "  ✓ MProto     → /home/prati/A2_Dist_NER_Agro/original_dsner/mproto/data/qtl\n",
      "  ✓ RoSTER     → /home/prati/A2_Dist_NER_Agro/original_dsner/RoSTER/data/qtl\n",
      "  ✓ SCDL       → /home/prati/A2_Dist_NER_Agro/original_dsner/SCDL/dataset\n"
     ]
    }
   ],
   "source": [
    "# Prepare for ALL methods\n",
    "prep = DataPreparer(DATA_DIR, PROJECT_ROOT, DATASET)\n",
    "results = prep.prepare_all()\n",
    "\n",
    "print(\"\\nData preparation results:\")\n",
    "print(\"═\" * 60)\n",
    "for method, path in results.items():\n",
    "    ok = \"✓\" if not str(path).startswith(\"ERROR\") else \"✗\"\n",
    "    print(f\"  {ok} {method:10s} → {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 13:12:25] INFO dsner_data: write_conll_dist: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl/train.ALL.txt\n",
      "[2026-02-27 13:12:25] INFO dsner_data: write_conll_bio: 18706 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl/train.txt\n",
      "[2026-02-27 13:12:25] INFO dsner_data: write_conll_bio: 1045 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl/test.txt\n",
      "[2026-02-27 13:12:25] INFO dsner_data: write_conll_bio: 25 sents → /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl/dev.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPuL data: /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/data/qtl\n",
      "  dev.txt                        9,077 bytes\n",
      "  test.txt                     267,134 bytes\n",
      "  train.ALL.txt              5,092,990 bytes\n",
      "  train.txt                  4,136,212 bytes\n"
     ]
    }
   ],
   "source": [
    "# Prepare for JUST CuPuL\n",
    "cupul_data_path = prep.prepare_for(\"CuPuL\")\n",
    "print(f\"CuPuL data: {cupul_data_path}\")\n",
    "\n",
    "for f in sorted(os.listdir(cupul_data_path)):\n",
    "    size = os.path.getsize(os.path.join(cupul_data_path, f))\n",
    "    print(f\"  {f:25s} {size:>10,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CuPuL train (first lines with entities) ===\n",
      "  shear O 1\n",
      "  force O 1\n",
      "  shear O 1\n",
      "  force O 1\n",
      "  meat O 1\n",
      "  tenderness O 1\n",
      "\n",
      "=== CuPuL test.txt (first entities) ===\n",
      "  lambing B-Trait\n",
      "  litter B-Trait\n",
      "  lambing B-Trait\n",
      "  lambing B-Trait\n"
     ]
    }
   ],
   "source": [
    "# Peek at CuPuL data\n",
    "print(\"=== CuPuL train (first lines with entities) ===\")\n",
    "# Check which train file exists\n",
    "for tname in [\"train.ALL.txt\", \"train.txt\"]:\n",
    "    tpath = os.path.join(cupul_data_path, tname)\n",
    "    if os.path.exists(tpath):\n",
    "        count = 0\n",
    "        with open(tpath) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 3 and parts[2] != \"0\":\n",
    "                    print(f\"  {line.strip()}\")\n",
    "                    count += 1\n",
    "                    if count >= 6:\n",
    "                        break\n",
    "                elif len(parts) >= 2 and parts[1] not in (\"O\", \"0\"):\n",
    "                    print(f\"  {line.strip()}\")\n",
    "                    count += 1\n",
    "                    if count >= 6:\n",
    "                        break\n",
    "        break\n",
    "\n",
    "print(\"\\n=== CuPuL test.txt (first entities) ===\")\n",
    "test_path = os.path.join(cupul_data_path, \"test.txt\")\n",
    "if os.path.exists(test_path):\n",
    "    count = 0\n",
    "    with open(test_path) as f:\n",
    "        for line in f:\n",
    "            if \"B-\" in line:\n",
    "                print(f\"  {line.strip()}\")\n",
    "                count += 1\n",
    "                if count >= 4:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad-hoc format conversion\n",
    "\n",
    "Convert between any pair of formats on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 13:12:25] INFO dsner_data: Auto-detected format: conll_bio\n",
      "[2026-02-27 13:12:25] INFO dsner_data: read_conll_bio: 1045 sentences from /home/prati/A2_Dist_NER_Agro/original_dsner/data/QTL/test.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1045 test sentences\n",
      "Tokens: ['Association', 'analysis', 'of', 'PLIN2', 'gene', 'polymorphisms', 'and', 'lambing']\n",
      "Tags  : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Trait']\n"
     ]
    }
   ],
   "source": [
    "fc = FormatConverter(types=TYPES)\n",
    "\n",
    "# Read from any format (auto-detects if fmt omitted)\n",
    "sents = fc.read(GOLD_TEST)\n",
    "print(f\"Read {len(sents)} test sentences\")\n",
    "print(f\"Tokens: {sents[0].tokens[:8]}\")\n",
    "print(f\"Tags  : {sents[0].bio_tags[:8]}\")\n",
    "\n",
    "# Example conversions (uncomment to run):\n",
    "# fc.convert(GOLD_TEST, \"/tmp/test.json\",  dst_fmt=\"bond_json\")\n",
    "# fc.convert(GOLD_TEST, \"/tmp/test.jsonl\", dst_fmt=\"mproto_jsonl\")\n",
    "# fc.convert(GOLD_TEST, \"/tmp/test.ck\",    dst_fmt=\"autoner_ck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Training & Inference\n",
    "\n",
    "### 3a. CuPuL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsner_wrapper import DSNERWrapper, CuPuLConfig, CONFIG_REGISTRY\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPuL config fields:\n",
      "───────────────────────────────────────────────────────\n",
      "  dataset                                = qtl\n",
      "  gpu_ids                                = 0\n",
      "  seed                                   = 0\n",
      "  max_seq_length                         = 128\n",
      "  train_batch_size                       = 32\n",
      "  eval_batch_size                        = 32\n",
      "  learning_rate                          = 2e-05\n",
      "  num_train_epochs                       = 1\n",
      "  output_dir                             = output\n",
      "  pretrained_model                       = roberta-base\n",
      "  tag_scheme                             = io\n",
      "  temp_dir                               = temp\n",
      "  do_train                               = True\n",
      "  do_eval                                = True\n",
      "  eval_on                                = test\n",
      "  train_on                               = train\n",
      "  loss_type                              = MAE\n",
      "  gradient_accumulation_steps            = 1\n",
      "  noise_train_update_interval            = 200\n",
      "  self_train_update_interval             = 100\n",
      "  warmup_proportion                      = 0.1\n",
      "  weight_decay                           = 0.01\n",
      "  curriculum_train_lr                    = 1e-05\n",
      "  curriculum_train_epochs                = 5\n",
      "  curriculum_train_sub_epochs            = 2\n",
      "  num_models                             = 5\n",
      "  self_train_lr                          = 1e-05\n",
      "  self_train_epochs                      = 10\n",
      "  student1_lr                            = 1e-05\n",
      "  student2_lr                            = 1e-05\n",
      "  drop_other                             = 0.1\n",
      "  drop_entity                            = 0.1\n",
      "  entity_threshold                       = 0.8\n",
      "  ratio                                  = 0.1\n",
      "  m                                      = 10.0\n",
      "  dict_dir                               = ./dictionaries\n"
     ]
    }
   ],
   "source": [
    "# Inspect CuPuL config defaults\n",
    "cfg_preview = CuPuLConfig(dataset=DATASET, gpu_ids=GPU)\n",
    "print(\"CuPuL config fields:\")\n",
    "print(\"─\" * 55)\n",
    "for f in dataclasses.fields(cfg_preview):\n",
    "    val = getattr(cfg_preview, f.name)\n",
    "    if f.name not in (\"extra\", \"dict_names\", \"tag2idx\") and val != \"\" and val != {}:\n",
    "        print(f\"  {f.name:38s} = {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 13:12:25] INFO dsner_wrapper: Initialized DSNERWrapper: method=CuPuL, dataset=qtl, gpu=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method dir : /home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL\n",
      "Dataset    : qtl\n",
      "GPU        : 0\n"
     ]
    }
   ],
   "source": [
    "# Create the CuPuL wrapper\n",
    "cupul = DSNERWrapper(\n",
    "    method=\"CuPuL\",\n",
    "    project_root=PROJECT_ROOT,\n",
    "    dataset=DATASET,\n",
    "    gpu_ids=GPU,\n",
    "\n",
    "    # ── Model ──\n",
    "    pretrained_model=\"roberta-base\",\n",
    "    tag_scheme=\"io\",\n",
    "    max_seq_length=300,\n",
    "\n",
    "    # ── Phase 1: Noise-robust training ──\n",
    "    learning_rate=5e-7,\n",
    "    num_train_epochs=1,\n",
    "    drop_other=0.5,\n",
    "    loss_type=\"MAE\",\n",
    "    m=20,\n",
    "    num_models=5,\n",
    "\n",
    "    # ── Phase 2: Curriculum training ──\n",
    "    curriculum_train_lr=2e-7,\n",
    "    curriculum_train_epochs=5,\n",
    "    curriculum_train_sub_epochs=1,\n",
    "\n",
    "    # ── Phase 3: Self-training ──\n",
    "    self_train_lr=5e-7,\n",
    "    self_train_epochs=5,\n",
    "    self_train_update_interval=100,\n",
    ")\n",
    "\n",
    "print(f\"Method dir : {cupul.runner.method_dir}\")\n",
    "print(f\"Dataset    : {cupul.config.dataset}\")\n",
    "print(f\"GPU        : {cupul.config.gpu_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command preview:\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "python train.py \\\n",
      "    --do_train \\\n",
      "    --do_eval \\\n",
      "    --dataset_name qtl \\\n",
      "    --pretrained_model roberta-base \\\n",
      "    --max_seq_length 300 \\\n",
      "    --tag_scheme io \\\n",
      "    --loss_type MAE \\\n",
      "    --train_batch_size 32 \\\n",
      "    --eval_batch_size 32 \\\n",
      "    --gradient_accumulation_steps 1 \\\n",
      "    --train_lr 5e-07 \\\n",
      "    --curriculum_train_lr 2e-07 \\\n",
      "    --train_epochs 1 \\\n",
      "    --curriculum_train_epochs 5 \\\n",
      "    --curriculum_train_sub_epochs 1 \\\n",
      "    --num_models 5 \\\n",
      "    --warmup_proportion 0.1 \\\n",
      "    --weight_decay 0.01 \\\n",
      "    --drop_other 0.5 \\\n",
      "    --drop_entity 0.1 \\\n",
      "    --seed 0 \\\n",
      "    --self_train_lr 5e-07 \\\n",
      "    --self_train_epochs 5 \\\n",
      "    --student1_lr 1e-05 \\\n",
      "    --student2_lr 1e-05 \\\n",
      "    --entity_threshold 0.8 \\\n",
      "    --ratio 0.1 \\\n",
      "    --m 20 \\\n",
      "    --noise_train_update_interval 200 \\\n",
      "    --self_train_update_interval 100 \\\n"
     ]
    }
   ],
   "source": [
    "# Preview the exact command\n",
    "parts = [\"python train.py --do_train --do_eval\"] + cupul.runner._build_common_args()\n",
    "cmd = \" \".join(parts)\n",
    "\n",
    "print(\"Command preview:\")\n",
    "print(\"═\" * 80)\n",
    "for part in cmd.split(\" --\"):\n",
    "    if part.startswith(\"python\"):\n",
    "        print(part + \" \\\\\")\n",
    "    else:\n",
    "        print(f\"    --{part} \\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 13:12:25] INFO dsner_wrapper: Starting training: CuPuL on qtl\n",
      "[2026-02-27 13:12:25] INFO dsner_wrapper: Running command:\n",
      "  cwd=/home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL\n",
      "  cmd=python train.py --do_train --do_eval --dataset_name qtl --pretrained_model roberta-base --max_seq_length 300 --tag_scheme io --loss_type MAE --train_batch_size 32 --eval_batch_size 32 --gradient_accumulation_steps 1 --train_lr 5e-07 --curriculum_train_lr 2e-07 --train_epochs 1 --curriculum_train_epochs 5 --curriculum_train_sub_epochs 1 --num_models 5 --warmup_proportion 0.1 --weight_decay 0.01 --drop_other 0.5 --drop_entity 0.1 --seed 0 --self_train_lr 5e-07 --self_train_epochs 5 --student1_lr 1e-05 --student2_lr 1e-05 --entity_threshold 0.8 --ratio 0.1 --m 20 --noise_train_update_interval 200 --self_train_update_interval 100\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 229, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 210, in main\n",
      "    classifier = NERClassifier(args)\n",
      "  File \"/home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/NERClassifier.py\", line 100, in __init__\n",
      "    self.risk = Risk(loss_type[args.dataset_name][\"voter\"], args.m, 0.5, self.num_labels, args.priors)\n",
      "KeyError: 'qtl'\n",
      "[2026-02-27 13:12:26] ERROR dsner_wrapper: Command failed with return code 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== args ==============\n",
      "\n",
      "curriculum_train_epochs 5\n",
      "curriculum_train_lr 2e-07\n",
      "curriculum_train_sub_epochs 1\n",
      "dataset_name qtl\n",
      "do_eval True\n",
      "do_train True\n",
      "drop_entity 0.1\n",
      "drop_other 0.5\n",
      "entity_threshold 0.8\n",
      "eval_batch_size 32\n",
      "eval_on test\n",
      "gradient_accumulation_steps 1\n",
      "loss_type MAE\n",
      "m 20.0\n",
      "max_seq_length 300\n",
      "noise_train_update_interval 200\n",
      "num_models 5\n",
      "output_dir ../data/qtl/output\n",
      "pretrained_model roberta-base\n",
      "priors [0.0314966102568, 0.0376880632424, 0.0354240324761, 0.015502139428]\n",
      "ratio 0.1\n",
      "seed 0\n",
      "self_train_epochs 5\n",
      "self_train_lr 5e-07\n",
      "self_train_update_interval 100\n",
      "student1_lr 1e-05\n",
      "student2_lr 1e-05\n",
      "tag_scheme io\n",
      "temp_dir ../data/qtl/temp\n",
      "train_batch_size 32\n",
      "train_epochs 1\n",
      "train_lr 5e-07\n",
      "train_on train\n",
      "warmup_proportion 0.1\n",
      "weight_decay 0.01\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python train.py --do_train --do_eval --dataset_name qtl --pretrained_model roberta-base --max_seq_length 300 --tag_scheme io --loss_type MAE --train_batch_size 32 --eval_batch_size 32 --gradient_accumulation_steps 1 --train_lr 5e-07 --curriculum_train_lr 2e-07 --train_epochs 1 --curriculum_train_epochs 5 --curriculum_train_sub_epochs 1 --num_models 5 --warmup_proportion 0.1 --weight_decay 0.01 --drop_other 0.5 --drop_entity 0.1 --seed 0 --self_train_lr 5e-07 --self_train_epochs 5 --student1_lr 1e-05 --student2_lr 1e-05 --entity_threshold 0.8 --ratio 0.1 --m 20 --noise_train_update_interval 200 --self_train_update_interval 100', returncode=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ╔═══════════════════════════════════════════════════════════════╗\n",
    "# ║              TRAIN CuPuL — uncomment to run                 ║\n",
    "# ╚═══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "cupul.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 13:12:26] INFO dsner_wrapper: Starting prediction: CuPuL\n",
      "[2026-02-27 13:12:26] INFO dsner_wrapper: Running command:\n",
      "  cwd=/home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL\n",
      "  cmd=python predict.py --dataset_name qtl --pretrained_model roberta-base --max_seq_length 300 --tag_scheme io --loss_type MAE --train_batch_size 32 --eval_batch_size 32 --gradient_accumulation_steps 1 --train_lr 5e-07 --curriculum_train_lr 2e-07 --train_epochs 1 --curriculum_train_epochs 5 --curriculum_train_sub_epochs 1 --num_models 5 --warmup_proportion 0.1 --weight_decay 0.01 --drop_other 0.5 --drop_entity 0.1 --seed 0 --self_train_lr 5e-07 --self_train_epochs 5 --student1_lr 1e-05 --student2_lr 1e-05 --entity_threshold 0.8 --ratio 0.1 --m 20 --noise_train_update_interval 200 --self_train_update_interval 100\n",
      "Traceback (most recent call last):\n",
      "  File \"predict.py\", line 210, in <module>\n",
      "    main()\n",
      "  File \"predict.py\", line 203, in main\n",
      "    trainer = NERClassifier(args)\n",
      "  File \"/home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL/NERClassifier.py\", line 100, in __init__\n",
      "    self.risk = Risk(loss_type[args.dataset_name][\"voter\"], args.m, 0.5, self.num_labels, args.priors)\n",
      "KeyError: 'qtl'\n",
      "[2026-02-27 13:12:28] ERROR dsner_wrapper: Command failed with return code 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== args ==============\n",
      "\n",
      "curriculum_train_epochs 5\n",
      "curriculum_train_lr 2e-07\n",
      "curriculum_train_sub_epochs 1\n",
      "dataset_name qtl\n",
      "do_eval False\n",
      "do_train False\n",
      "drop_entity 0.1\n",
      "drop_other 0.5\n",
      "entity_threshold 0.8\n",
      "eval_batch_size 32\n",
      "eval_on test\n",
      "gradient_accumulation_steps 1\n",
      "loss_type MAE\n",
      "m 20.0\n",
      "max_seq_length 300\n",
      "noise_train_update_interval 200\n",
      "num_models 5\n",
      "output_dir ../data/qtl/output\n",
      "pretrained_model roberta-base\n",
      "priors [0.0314966102568, 0.0376880632424, 0.0354240324761, 0.015502139428]\n",
      "ratio 0.1\n",
      "seed 0\n",
      "self_train_epochs 5\n",
      "self_train_lr 5e-07\n",
      "self_train_update_interval 100\n",
      "student1_lr 1e-05\n",
      "student2_lr 1e-05\n",
      "tag_scheme io\n",
      "temp_dir ../data/qtl/temp\n",
      "train_batch_size 32\n",
      "train_epochs 1\n",
      "train_lr 5e-07\n",
      "train_on train\n",
      "warmup_proportion 0.1\n",
      "weight_decay 0.01\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python predict.py --dataset_name qtl --pretrained_model roberta-base --max_seq_length 300 --tag_scheme io --loss_type MAE --train_batch_size 32 --eval_batch_size 32 --gradient_accumulation_steps 1 --train_lr 5e-07 --curriculum_train_lr 2e-07 --train_epochs 1 --curriculum_train_epochs 5 --curriculum_train_sub_epochs 1 --num_models 5 --warmup_proportion 0.1 --weight_decay 0.01 --drop_other 0.5 --drop_entity 0.1 --seed 0 --self_train_lr 5e-07 --self_train_epochs 5 --student1_lr 1e-05 --student2_lr 1e-05 --entity_threshold 0.8 --ratio 0.1 --m 20 --noise_train_update_interval 200 --self_train_update_interval 100', returncode=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ╔═══════════════════════════════════════════════════════════════╗\n",
    "# ║            PREDICT CuPuL — uncomment to run                 ║\n",
    "# ╚═══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "cupul.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 13:12:28] INFO dsner_wrapper: Starting evaluation: CuPuL on qtl\n",
      "[2026-02-27 13:12:28] INFO dsner_wrapper: Running command:\n",
      "  cwd=/home/prati/A2_Dist_NER_Agro/original_dsner/CuPuL\n",
      "  cmd=python train.py --do_eval --dataset_name qtl --pretrained_model roberta-base --max_seq_length 300 --tag_scheme io --loss_type MAE --train_batch_size 32 --eval_batch_size 32 --gradient_accumulation_steps 1 --train_lr 5e-07 --curriculum_train_lr 2e-07 --train_epochs 1 --curriculum_train_epochs 5 --curriculum_train_sub_epochs 1 --num_models 5 --warmup_proportion 0.1 --weight_decay 0.01 --drop_other 0.5 --drop_entity 0.1 --seed 0 --self_train_lr 5e-07 --self_train_epochs 5 --student1_lr 1e-05 --student2_lr 1e-05 --entity_threshold 0.8 --ratio 0.1 --m 20 --noise_train_update_interval 200 --self_train_update_interval 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== args ==============\n",
      "\n",
      "curriculum_train_epochs 5\n",
      "curriculum_train_lr 2e-07\n",
      "curriculum_train_sub_epochs 1\n",
      "dataset_name qtl\n",
      "do_eval True\n",
      "do_train False\n",
      "drop_entity 0.1\n",
      "drop_other 0.5\n",
      "entity_threshold 0.8\n",
      "eval_batch_size 32\n",
      "eval_on test\n",
      "gradient_accumulation_steps 1\n",
      "loss_type MAE\n",
      "m 20.0\n",
      "max_seq_length 300\n",
      "noise_train_update_interval 200\n",
      "num_models 5\n",
      "output_dir ../data/qtl/output\n",
      "pretrained_model roberta-base\n",
      "priors [0.0314966102568, 0.0376880632424, 0.0354240324761, 0.015502139428]\n",
      "ratio 0.1\n",
      "seed 0\n",
      "self_train_epochs 5\n",
      "self_train_lr 5e-07\n",
      "self_train_update_interval 100\n",
      "student1_lr 1e-05\n",
      "student2_lr 1e-05\n",
      "tag_scheme io\n",
      "temp_dir ../data/qtl/temp\n",
      "train_batch_size 32\n",
      "train_epochs 1\n",
      "train_lr 5e-07\n",
      "train_on train\n",
      "warmup_proportion 0.1\n",
      "weight_decay 0.01\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python train.py --do_eval --dataset_name qtl --pretrained_model roberta-base --max_seq_length 300 --tag_scheme io --loss_type MAE --train_batch_size 32 --eval_batch_size 32 --gradient_accumulation_steps 1 --train_lr 5e-07 --curriculum_train_lr 2e-07 --train_epochs 1 --curriculum_train_epochs 5 --curriculum_train_sub_epochs 1 --num_models 5 --warmup_proportion 0.1 --weight_decay 0.01 --drop_other 0.5 --drop_entity 0.1 --seed 0 --self_train_lr 5e-07 --self_train_epochs 5 --student1_lr 1e-05 --student2_lr 1e-05 --entity_threshold 0.8 --ratio 0.1 --m 20 --noise_train_update_interval 200 --self_train_update_interval 100', returncode=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ╔═══════════════════════════════════════════════════════════════╗\n",
    "# ║           EVALUATE CuPuL — uncomment to run                 ║\n",
    "# ╚═══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "cupul.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. BOND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bond = DSNERWrapper(\n",
    "#     method=\"BOND\",\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     dataset=DATASET,\n",
    "#     gpu_ids=GPU,\n",
    "#     model_type=\"roberta\",\n",
    "#     model_name_or_path=\"roberta-base\",\n",
    "#     learning_rate=5e-5,\n",
    "#     num_train_epochs=3,\n",
    "#     max_seq_length=128,\n",
    "#     self_training=True,\n",
    "#     mt=1,\n",
    "#     mt_updatefreq=5,\n",
    "# )\n",
    "# bond.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. RoSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roster = DSNERWrapper(\n",
    "#     method=\"RoSTER\",\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     dataset=DATASET,\n",
    "#     gpu_ids=GPU,\n",
    "#     pretrained_model=\"roberta-base\",\n",
    "#     tag_scheme=\"io\",\n",
    "#     max_seq_length=300,\n",
    "#     noise_train_lr=3e-5,\n",
    "#     noise_train_epochs=3,\n",
    "#     ensemble_train_lr=1e-5,\n",
    "#     ensemble_train_epochs=2,\n",
    "#     self_train_lr=5e-7,\n",
    "#     self_train_epochs=5,\n",
    "#     q=0.7,\n",
    "#     tau=0.7,\n",
    "#     seed=30,\n",
    "# )\n",
    "# roster.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. ATSEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atsen = DSNERWrapper(\n",
    "#     method=\"ATSEN\",\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     dataset=DATASET,\n",
    "#     gpu_ids=GPU,\n",
    "#     learning_rate=1e-5,\n",
    "#     warmup_steps=200,\n",
    "#     begin_epoch=1,\n",
    "#     period=6000,\n",
    "#     threshold=0.9,\n",
    "#     num_train_epochs=50,\n",
    "#     al=0.8,\n",
    "#     bate=1.0,\n",
    "# )\n",
    "# atsen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. DeSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desert = DSNERWrapper(\n",
    "#     method=\"DeSERT\",\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     dataset=DATASET,\n",
    "#     gpu_ids=GPU,\n",
    "#     threshold=0.9,\n",
    "#     num_train_epochs=50,\n",
    "# )\n",
    "# desert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. SCDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scdl = DSNERWrapper(\n",
    "#     method=\"SCDL\",\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     dataset=DATASET,\n",
    "#     gpu_ids=GPU,\n",
    "#     learning_rate=2e-5,\n",
    "#     begin_epoch=6,\n",
    "#     period=3200,\n",
    "#     num_train_epochs=50,\n",
    "# )\n",
    "# scdl.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3g. MProto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mproto = DSNERWrapper(\n",
    "#     method=\"MProto\",\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     dataset=DATASET,\n",
    "#     gpu_ids=GPU,\n",
    "#     config_path=\"cfg/qtl/mproto/train.toml\",\n",
    "# )\n",
    "# mproto.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3h. AutoNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoner = DSNERWrapper(\n",
    "#     method=\"AutoNER\",\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     dataset=DATASET,\n",
    "#     model_name=DATASET,\n",
    "#     hid_dim=300,\n",
    "#     word_dim=200,\n",
    "#     num_train_epochs=50,\n",
    "#     learning_rate=0.05,\n",
    "#     optimizer=\"SGD\",\n",
    "# )\n",
    "# autoner.train()\n",
    "# autoner.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Post-Processing\n",
    "\n",
    "\n",
    "| Rule | What it does |\n",
    "|------|--------|\n",
    "|`span_consistency` | All case-insensitive matches of entity spans → entity |\n",
    "|`prep_bridging` | [ent] of [ent] → bridge the preposition |\n",
    "|`abbrev_resolution` | \"body weight (BW)\" → propagate labels |\n",
    "|`pos_filtering` | Demote singleton non-noun predictions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available rules: ['abbrev_resolution', 'pos_filtering', 'prep_bridging', 'span_consistency']\n",
      "Default order  : span_consistency → prep_bridging → span_consistency → abbrev_resolution → span_consistency → pos_filtering\n"
     ]
    }
   ],
   "source": [
    "from dsner_postprocess import (\n",
    "    PostProcessor,\n",
    "    collect_predictions,\n",
    "    evaluate_predictions,\n",
    "    read_predictions,\n",
    "    write_predictions,\n",
    "    collect_from_conll_pred,\n",
    "    AVAILABLE_RULES,\n",
    "    DEFAULT_RULE_ORDER,\n",
    ")\n",
    "\n",
    "print(\"Available rules:\", sorted(AVAILABLE_RULES))\n",
    "print(f\"Default order  : {' → '.join(DEFAULT_RULE_ORDER)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Post-process an existing prediction file\n",
    "\n",
    "If you already have a prediction file in `TOKEN GOLD_TAG PRED_INT` format (e.g. `pred_test_high_recall.txt`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found prediction files:\n",
      "  [0] pred_test_balance.txt                      33,267 lines\n",
      "  [1] pred_test_high_recall.txt                  33,268 lines\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect existing prediction files in the data directory\n",
    "pred_files = []\n",
    "if DATA_DIR:\n",
    "    for f in os.listdir(DATA_DIR):\n",
    "        if f.startswith(\"pred_\") and f.endswith(\".txt\"):\n",
    "            pred_files.append(os.path.join(DATA_DIR, f))\n",
    "\n",
    "if pred_files:\n",
    "    print(\"Found prediction files:\")\n",
    "    for i, pf in enumerate(pred_files):\n",
    "        n = sum(1 for _ in open(pf))\n",
    "        print(f\"  [{i}] {os.path.basename(pf):40s} {n:>8,} lines\")\n",
    "else:\n",
    "    print(\"No prediction files found in data directory.\")\n",
    "    print(\"Run training first, or place a prediction file in:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set PRED_FILE and uncomment the cells below\n"
     ]
    }
   ],
   "source": [
    "# ── Pick a prediction file to post-process ──\n",
    "# Option 1: Use one of the auto-detected files\n",
    "# PRED_FILE = pred_files[0]  # e.g. pred_test_high_recall.txt\n",
    "\n",
    "# Option 2: Explicit path\n",
    "# PRED_FILE = os.path.join(DATA_DIR, \"pred_test_high_recall.txt\")\n",
    "\n",
    "# Uncomment one of the above, then run the cells below\n",
    "print(\"Set PRED_FILE and uncomment the cells below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Evaluate BEFORE post-processing ──\n",
    "\n",
    "# pp = PostProcessor(PRED_FILE)\n",
    "# print(f\"Loaded: {pp.summary()}\")\n",
    "# print()\n",
    "# results_before = pp.evaluate(TYPES)\n",
    "# print(f\"Before (strict):  P={results_before['strict_precision']:.4f}  R={results_before['strict_recall']:.4f}  F1={results_before['strict_f1']:.4f}\")# print(f\"Before (relaxed):  P={results_before['relaxed_precision']:.4f}  R={results_before['relaxed_recall']:.4f}  F1={results_before['relaxed_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Apply ALL rules ──\n",
    "\n",
    "# pp.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Evaluate AFTER ──\n",
    "\n",
    "# results_after = pp.evaluate(TYPES)\n",
    "# print(f\"After (strict):  P={results_after['strict_precision']:.4f}  R={results_after['strict_recall']:.4f}  F1={results_after['strict_f1']:.4f}\")\n",
    "# print(f\"After (relaxed):  P={results_after['relaxed_precision']:.4f}  R={results_after['relaxed_recall']:.4f}  F1={results_after['relaxed_f1']:.4f}\")\n",
    "# delta_s = results_after['strict_f1'] - results_before['strict_f1']\n",
    "# delta_r = results_after['relaxed_f1'] - results_before['relaxed_f1']\n",
    "# print(f\"F1 Δ strict={'+' if delta_s >= 0 else ''}{delta_s:.4f}  relaxed={'+' if delta_r >= 0 else ''}{delta_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save ──\n",
    "\n",
    "# output_file = os.path.join(PRED_DIR, \"pred_final.txt\")\n",
    "# pp.save(output_file)\n",
    "# print(f\"Saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Collect predictions from a trained method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training a method, collect its predictions into unified format:\n",
    "\n",
    "# unified_file = collect_predictions(\n",
    "#     method=\"CuPuL\",              # or \"BOND\", \"RoSTER\", etc.\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     dataset=DATASET,\n",
    "#     gold_file=GOLD_TEST,\n",
    "#     types=TYPES,\n",
    "#     output=os.path.join(PRED_DIR, \"cupul_unified.txt\"),\n",
    "# )\n",
    "\n",
    "# # Then post-process\n",
    "# pp = PostProcessor(unified_file)\n",
    "# pp.run_all()\n",
    "# pp.evaluate(TYPES)\n",
    "# pp.save(os.path.join(PRED_DIR, \"cupul_final.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Custom rule selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply only specific rules:\n",
    "# pp2 = PostProcessor(PRED_FILE)\n",
    "# pp2.run(rules=[\"prep_bridging\", \"span_consistency\"])\n",
    "# pp2.evaluate(TYPES)\n",
    "\n",
    "# Custom trait words / prepositions:\n",
    "# pp3 = PostProcessor(PRED_FILE,\n",
    "#     prepositions={\"of\", \"in\", \"for\", \"to\", \"with\"},\n",
    "#     noun_tags={\"NN\", \"NNS\", \"NNP\", \"NNPS\"},\n",
    "# )\n",
    "# pp3.run_all()\n",
    "# pp3.evaluate(TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Batch Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_method() defined\n"
     ]
    }
   ],
   "source": [
    "def run_method(method_name, wrapper_kwargs, pred_file=None):\n",
    "    \"\"\"Full pipeline: train → predict → collect → postprocess → evaluate.\"\"\"\n",
    "    import traceback\n",
    "\n",
    "    print(f\"\\n{'═' * 60}\")\n",
    "    print(f\"  Running: {method_name}\")\n",
    "    print(f\"{'═' * 60}\")\n",
    "\n",
    "    try:\n",
    "        w = DSNERWrapper(\n",
    "            method=method_name,\n",
    "            project_root=PROJECT_ROOT,\n",
    "            dataset=DATASET,\n",
    "            gpu_ids=GPU,\n",
    "            **wrapper_kwargs,\n",
    "        )\n",
    "        w.train()\n",
    "        try:\n",
    "            w.predict()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        out = os.path.join(PRED_DIR, f\"{method_name.lower()}_{DATASET}_unified.txt\")\n",
    "        unified = collect_predictions(\n",
    "            method=method_name, project_root=PROJECT_ROOT, dataset=DATASET,\n",
    "            gold_file=GOLD_TEST, pred_file=pred_file, types=TYPES, output=out,\n",
    "        )\n",
    "\n",
    "        raw = evaluate_predictions(read_predictions(unified), TYPES)\n",
    "        pp = PostProcessor(unified)\n",
    "        pp.run_all()\n",
    "        pp_res = pp.evaluate(TYPES)\n",
    "        pp.save(out.replace(\"_unified\", \"_final\"))\n",
    "\n",
    "        return {\n",
    "            \"method\": method_name,\n",
    "            \"raw_strict_P\": raw[\"strict_precision\"], \"raw_strict_R\": raw[\"strict_recall\"], \"raw_strict_F1\": raw[\"strict_f1\"],\n",
    "            \"pp_strict_P\": pp_res[\"strict_precision\"], \"pp_strict_R\": pp_res[\"strict_recall\"], \"pp_strict_F1\": pp_res[\"strict_f1\"],\n",
    "            \"raw_relaxed_P\": raw[\"relaxed_precision\"], \"raw_relaxed_R\": raw[\"relaxed_recall\"], \"raw_relaxed_F1\": raw[\"relaxed_f1\"],\n",
    "            \"pp_relaxed_P\": pp_res[\"relaxed_precision\"], \"pp_relaxed_R\": pp_res[\"relaxed_recall\"], \"pp_relaxed_F1\": pp_res[\"relaxed_f1\"],\n",
    "            \"status\": \"ok\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return {\"method\": method_name, \"status\": f\"FAILED: {e}\"}\n",
    "\n",
    "print(\"run_method() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Uncomment methods to compare ──\n",
    "\n",
    "# all_results = []\n",
    "\n",
    "# all_results.append(run_method(\"CuPuL\", {\n",
    "#     \"pretrained_model\": \"roberta-base\", \"tag_scheme\": \"io\", \"max_seq_length\": 300,\n",
    "#     \"learning_rate\": 5e-7, \"num_train_epochs\": 1, \"drop_other\": 0.5,\n",
    "#     \"loss_type\": \"MAE\", \"m\": 20,\n",
    "#     \"curriculum_train_lr\": 2e-7, \"curriculum_train_epochs\": 5,\n",
    "#     \"self_train_lr\": 5e-7, \"self_train_epochs\": 5,\n",
    "# }))\n",
    "\n",
    "# all_results.append(run_method(\"RoSTER\", {\n",
    "#     \"pretrained_model\": \"roberta-base\", \"tag_scheme\": \"io\", \"max_seq_length\": 300,\n",
    "#     \"noise_train_lr\": 3e-5, \"noise_train_epochs\": 3,\n",
    "#     \"ensemble_train_lr\": 1e-5, \"self_train_lr\": 5e-7, \"self_train_epochs\": 5,\n",
    "#     \"seed\": 30,\n",
    "# }))\n",
    "\n",
    "# all_results.append(run_method(\"BOND\", {\n",
    "#     \"model_type\": \"roberta\", \"model_name_or_path\": \"roberta-base\",\n",
    "#     \"learning_rate\": 5e-5, \"num_train_epochs\": 3, \"self_training\": True, \"mt\": 1,\n",
    "# }))\n",
    "\n",
    "# all_results.append(run_method(\"ATSEN\", {\n",
    "#     \"learning_rate\": 1e-5, \"num_train_epochs\": 50,\n",
    "#     \"begin_epoch\": 1, \"period\": 6000, \"threshold\": 0.9, \"al\": 0.8, \"bate\": 1.0,\n",
    "# }))\n",
    "\n",
    "# all_results.append(run_method(\"DeSERT\", {\"threshold\": 0.9, \"num_train_epochs\": 50}))\n",
    "\n",
    "# all_results.append(run_method(\"SCDL\", {\n",
    "#     \"learning_rate\": 2e-5, \"begin_epoch\": 6, \"period\": 3200, \"num_train_epochs\": 50,\n",
    "# }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Display comparison table ──\n",
    "\n",
    "# import pandas as pd\n",
    "# ok_results = [r for r in all_results if r.get(\"status\") == \"ok\"]\n",
    "# if ok_results:\n",
    "#     df = pd.DataFrame(ok_results)\n",
    "#     cols = [\"method\", \"raw_P\", \"raw_R\", \"raw_F1\", \"pp_P\", \"pp_R\", \"pp_F1\"]\n",
    "#     df = df[[c for c in cols if c in df.columns]].sort_values(\"pp_relaxed_F1\", ascending=False)\n",
    "#     print(df.to_string(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ATSEN        32 config fields\n",
      "  AutoNER      35 config fields\n",
      "  BOND         57 config fields\n",
      "  CuPuL        39 config fields\n",
      "  DeSERT       34 config fields\n",
      "  MProto       15 config fields\n",
      "  RoSTER       31 config fields\n",
      "  SCDL         30 config fields\n"
     ]
    }
   ],
   "source": [
    "# List all methods and config sizes\n",
    "for method, cfg_cls in CONFIG_REGISTRY.items():\n",
    "    c = cfg_cls(dataset=DATASET, gpu_ids=GPU)\n",
    "    n = len(dataclasses.fields(c))\n",
    "    print(f\"  {method:10s}  {n:3d} config fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPuL config:\n",
      "───────────────────────────────────────────────────────\n",
      "  dataset                                = qtl\n",
      "  gpu_ids                                = 0\n",
      "  seed                                   = 0\n",
      "  max_seq_length                         = 128\n",
      "  train_batch_size                       = 32\n",
      "  eval_batch_size                        = 32\n",
      "  learning_rate                          = 2e-05\n",
      "  num_train_epochs                       = 1\n",
      "  output_dir                             = output\n",
      "  pretrained_model                       = roberta-base\n",
      "  tag_scheme                             = io\n",
      "  temp_dir                               = temp\n",
      "  do_train                               = True\n",
      "  do_eval                                = True\n",
      "  eval_on                                = test\n",
      "  train_on                               = train\n",
      "  loss_type                              = MAE\n",
      "  gradient_accumulation_steps            = 1\n",
      "  noise_train_update_interval            = 200\n",
      "  self_train_update_interval             = 100\n",
      "  warmup_proportion                      = 0.1\n",
      "  weight_decay                           = 0.01\n",
      "  curriculum_train_lr                    = 1e-05\n",
      "  curriculum_train_epochs                = 5\n",
      "  curriculum_train_sub_epochs            = 2\n",
      "  num_models                             = 5\n",
      "  self_train_lr                          = 1e-05\n",
      "  self_train_epochs                      = 10\n",
      "  student1_lr                            = 1e-05\n",
      "  student2_lr                            = 1e-05\n",
      "  drop_other                             = 0.1\n",
      "  drop_entity                            = 0.1\n",
      "  entity_threshold                       = 0.8\n",
      "  ratio                                  = 0.1\n",
      "  m                                      = 10.0\n",
      "  dict_dir                               = ./dictionaries\n",
      "  dict_names                             = {}\n",
      "  tag2idx                                = {}\n"
     ]
    }
   ],
   "source": [
    "# Inspect any method's full config\n",
    "METHOD_TO_INSPECT = \"CuPuL\"  # change to any method\n",
    "\n",
    "cfg_cls = CONFIG_REGISTRY[METHOD_TO_INSPECT]\n",
    "c = cfg_cls(dataset=DATASET, gpu_ids=GPU)\n",
    "print(f\"{METHOD_TO_INSPECT} config:\")\n",
    "print(\"─\" * 55)\n",
    "for f in dataclasses.fields(c):\n",
    "    val = getattr(c, f.name)\n",
    "    if f.name not in (\"extra\",):\n",
    "        print(f\"  {f.name:38s} = {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluate any 3-column prediction file\n",
    "# tokens = read_predictions(\"/path/to/pred.txt\")\n",
    "# evaluate_predictions(tokens, TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference\n",
    "\n",
    "```\n",
    "project_root/\n",
    "├── dsner_wrapper.py          # 8 method configs + runners\n",
    "├── dsner_data.py             # 8 format converters\n",
    "├── dsner_postprocess.py      # 5 post-processing rules + eval\n",
    "├── dsner_jupyter.ipynb       # this notebook\n",
    "├── data/QTL/                 # input data\n",
    "│   ├── train.txt / train_ALL.txt\n",
    "│   ├── test.txt\n",
    "│   ├── valid.txt\n",
    "│   └── types.txt\n",
    "├── predictions/              # output (auto-created)\n",
    "├── ATSEN/  BOND/  CuPuL/  DeSERT/\n",
    "├── RoSTER/  SCDL/  AutoNER/  mproto/\n",
    "└── ...\n",
    "\n",
    "Pipeline:\n",
    "  data/QTL/ → DataPreparer → method-specific format\n",
    "           → DSNERWrapper.train()\n",
    "           → DSNERWrapper.predict()\n",
    "           → collect_predictions()\n",
    "           → PostProcessor.run_all()\n",
    "           → P / R / F1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsner)",
   "language": "python",
   "name": "dsner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
